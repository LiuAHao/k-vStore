### 基于 Raft 的分布式键值存储系统 C++ 框架设计

#### 整体架构示意图
```
+----------------+       RPC 通信        +----------------+
|   Client       | <------------------> |   KvServer     |
+----------------+                       +----------------+
                                           |  |  ▲
                                           |  |  |
                                           v  v  |
+----------------+       Raft 协议通信     +----------------+
|   RaftNode     | <------------------> |   RaftNode     |
+----------------+                       +----------------+
```

---

### 核心类定义及功能说明

#### 1. `RaftNode` 类 - Raft 算法核心实现
```cpp
class RaftNode {
private:
    // Raft 持久化状态（需要落盘）
    PersistentStorage storage; // 持久化存储对象
    int currentTerm;           // 当前任期号
    int votedFor;              // 本任期投票给谁
    vector<LogEntry> log;      // 操作日志
    
    // Raft 易失性状态
    int commitIndex;           // 已提交的日志索引
    int lastApplied;           // 最后应用到状态机的索引
    map<int, int> nextIndex;   // 每个 follower 的下一个发送位置
    
    // 网络通信
    RpcClient rpcClient;       // RPC 客户端，用于向其他节点发送请求
    RpcServer rpcServer;       // RPC 服务端，接收其他节点请求
    
    // 状态机
    StateMachine* stateMachine; // 指向具体状态机（如键值存储）
    
public:
    void startElection();      // 发起选举
    void handleHeartbeat();     // 领导者发送心跳
    void appendEntriesRPC();   // 处理日志追加请求
    void requestVoteRPC();     // 处理投票请求
    void applyCommittedLogs();  // 将已提交日志应用到状态机
};
```

#### 2. `KvServer` 类 - 键值服务接口层
```cpp
class KvServer {
private:
    RaftNode& raftNode;        // 关联的 Raft 节点
    KvStateMachine stateMachine; // 键值状态机
    
public:
    // 客户端接口
    string get(const string& key);
    void set(const string& key, const string& value);
    void deleteKey(const string& key);
    
    // RPC 处理接口
    void handleClientRequest(const Request& req);
};
```

#### 3. `KvStateMachine` 类 - 键值存储状态机
```cpp
class KvStateMachine : public StateMachine {
private:
    // 本地存储引擎（可选择 LevelDB/RocksDB/SQLite）
    DB* database; 
    
public:
    void apply(const LogEntry& entry) override {
        // 执行具体操作：SET/GET/DELETE
        if (entry.command.type == "SET") {
            database->put(entry.key, entry.value);
        } else if (...) {
            // 处理其他操作
        }
    }
    
    string getSnapshot() { /* 生成快照 */ }
    void applySnapshot() { /* 从快照恢复 */ }
};
```

#### 4. `PersistentStorage` 类 - 持久化存储
```cpp
class PersistentStorage {
public:
    void saveTerm(int term);         // 保存当前任期
    void saveVote(int candidateId);  // 保存投票记录
    void saveLogs(const vector<LogEntry>& logs); // 保存日志
    
    int loadTerm();                  // 加载持久化数据
    vector<LogEntry> loadLogs();
};
```

#### 5. `RpcClient` / `RpcServer` 类 - 网络通信层
```cpp
class RpcClient {
public:
    void sendRequestVote(RaftNode* target);
    void sendAppendEntries(RaftNode* target);
};

class RpcServer {
public:
    void registerHandler(RequestVoteHandler handler);
    void startListening(int port);
};
```

#### 6. LogEntry 类 - 日志条目
```cpp
// 操作类型枚举
enum class Operation {
    SET,
    GET,
    DELETE,
    NOOP // 用于领导者心跳的空操作
};

class LogEntry {
private:
    int term;          // 日志对应的任期号（Raft 关键字段）
    int index;         // 日志在全局的顺序索引
    Operation op;      // 操作类型
    string key;        // 操作的键（GET/SET/DELETE 时使用）
    string value;      // 操作的值（仅 SET 时使用）
    string clientId;   // 客户端标识（用于去重）
    int64_t requestId; // 客户端请求唯一ID（用于幂等性）

public:
    // 序列化方法（用于网络传输和持久化）
    std::string serialize() const {
        // 可使用 Protocol Buffers、JSON 或二进制打包
        // 示例伪代码（实际实现需要选择序列化库）：
        return to_string(term) + "|" 
             + to_string(index) + "|"
             + to_string(static_cast<int>(op)) + "|"
             + key + "|" + value;
    }

    // 反序列化方法
    static LogEntry deserialize(const std::string& data) {
        // 解析字符串并填充字段
    }

    // Getters
    int getTerm() const { return term; }
    Operation getOp() const { return op; }
    // ...其他字段的getter
};

---

### 数据库设计建议

1. **不需要引入 MySQL**  
   分布式系统本身已具备数据可靠性，建议使用：
   - **嵌入式键值数据库**：LevelDB / RocksDB（推荐）
     ```cpp
     #include <leveldb/db.h>
     leveldb::DB* db;
     leveldb::Options options;
     options.create_if_missing = true;
     leveldb::DB::Open(options, "/data/kvstore", &db);
     ```
   - **内存存储 + 快照持久化**（适合小数据量）

2. **存储路径规划**  
   在 Ubuntu 系统下可部署在以下位置：
   ```
   /var/raftkv/               # 主目录
     ├── node1/               # 节点1数据
     │   ├── logs             # Raft 日志
     │   └── snapshots        # 状态机快照
     ├── node2/               # 节点2数据
     └── ...
   ```

3. **快照机制**  
   定期将状态机的当前状态（数据库内容）生成快照，减少日志重放时间。

---

### 关键执行流程示例

**客户端写入操作流程**：
1. 客户端调用 `KvServer::set("key", "value")`
2. `KvServer` 将操作封装为 LogEntry，调用 `RaftNode::propose(entry)`
3. Raft 节点通过 `appendEntriesRPC` 在集群中达成共识
4. 当日志提交后，`applyCommittedLogs` 将操作应用到 `KvStateMachine`
5. 状态机通过 LevelDB 执行实际存储

**领导者选举流程**：
1. Follower 检测到选举超时
2. 转换为 Candidate，调用 `startElection()`
3. 通过 `requestVoteRPC` 向其他节点拉票
4. 获得多数票后成为 Leader，开始发送心跳

--- 

### 详细架构说明：网络通信层与键值服务层

---

#### 一、RpcClient/RpcServer 详细设计（网络通信核心）

##### 1. `RpcServer` 类 - RPC 服务端
```cpp
class RpcServer {
private:
    // RPC 服务端核心组件
    grpc::ServerBuilder builder;       // 使用 gRPC 作为示例
    std::unique_ptr<grpc::Server> server;
    int port;                          // 监听端口
    
    // 请求处理回调接口
    std::function<void(const RequestVoteReq&, RequestVoteResp&)> voteHandler;
    std::function<void(const AppendEntriesReq&, AppendEntriesResp&)> appendHandler;
    std::function<void(const ClientRequest&, ClientResponse&)> clientCmdHandler;

public:
    // 启动服务端
    void start() {
        builder.AddListeningPort("0.0.0.0:"+std::to_string(port), 
            grpc::InsecureServerCredentials());
        builder.RegisterService(this); // 注册服务
        server = builder.BuildAndStart();
    }

    // gRPC 服务方法实现（自动生成部分代码）
    grpc::Status RequestVote(grpc::ServerContext* ctx, 
                            const RequestVoteReq* req, 
                            RequestVoteResp* resp) override {
            voteHandler(*req, *resp); // 转交给 RaftNode 处理
            return grpc::Status::OK;
        }
    };
```

##### 2. `RpcClient` 类 - RPC 客户端
```cpp
class RpcClient {
private:
    struct PeerConnection {
        std::string address; // 如 "10.0.0.2:50051"
        std::unique_ptr<raftRpc::Stub> stub; // gRPC 生成的存根
    };
    std::vector<PeerConnection> peers; // 集群所有节点连接信息

public:
    // 发送请求投票 RPC
    bool sendRequestVote(int peerId, const RequestVoteReq& req, 
                        RequestVoteResp& resp) {
        grpc::ClientContext ctx;
        auto status = peers[peerId].stub->RequestVote(&ctx, req, &resp);
        return status.ok(); // 返回网络是否成功
    }

    // 发送客户端命令到 Leader
    bool sendClientCommand(const std::string& leaderAddr, 
                          const ClientRequest& req,
                          ClientResponse& resp) {
        auto channel = grpc::CreateChannel(leaderAddr, 
            grpc::InsecureChannelCredentials());
        auto stub = KvService::NewStub(channel);
        return stub->ClientCommand(&ctx, req, &resp).ok();
    }
};
```

##### 3. 关键设计要点
- **连接管理**：维护所有节点的长连接（避免频繁创建）
- **超时控制**：所有 RPC 调用需设置超时（例如 150ms）
- **重试机制**：对失败请求进行有限次重试
- **线程模型**：建议采用 **IO 多线程 + 工作线程池**
  ```cpp
  // gRPC 服务端配置示例
  builder.SetSyncServerOption(grpc::ServerBuilder::NUM_CQS, 4); // 4个完成队列
  builder.SetSyncServerOption(grpc::ServerBuilder::MIN_POLLERS, 2); // 线程数
  ```

---

#### 二、KvServer 详细设计（业务接口层）

##### 1. 类定义与核心方法
```cpp
class KvServer {
private:
    RaftNode& raftNode;          // 关联的 Raft 节点
    KvStateMachine stateMachine; // 键值存储状态机
    std::mutex pendingReqMutex;  
    std::map<int64_t, std::promise<std::string>> pendingRequests; // 未完成请求
    
public:
    // 处理客户端 GET 请求（线性化读取）
    std::string get(const std::string& key) {
        // 1. 如果当前节点是 Leader，直接查询状态机
        if (raftNode.isLeader()) {
            return stateMachine.get(key); 
        }
        // 2. 否则转发请求到 Leader
        else {
            auto leaderAddr = raftNode.getLeaderAddress();
            return rpcClient.sendClientCommand(leaderAddr, ...);
        }
    }

    // 处理客户端 SET 请求（需要走 Raft 日志）
    void set(const std::string& key, const std::string& value) {
        LogEntry entry;
        entry.op = Operation::SET;
        entry.key = key;
        entry.value = value;
        entry.clientId = ...; // 客户端唯一标识
        
        // 提交到 Raft 集群
        int64_t logIndex = raftNode.propose(entry); 
        
        // 等待日志提交（异步等待）
        auto fut = waitForCommit(logIndex); 
        fut.get(); // 阻塞直到提交或超时
    }

    // 等待指定日志索引被提交
    std::future<void> waitForCommit(int64_t index) {
        std::promise<void> promise;
        {
            std::lock_guard<std::mutex> lk(pendingReqMutex);
            pendingRequests[index] = std::move(promise);
        }
        return promise.get_future();
    }
};
```

##### 2. 与 Raft 的交互流程
```
客户端请求处理流程：
1. 客户端发起 SET 请求到任意 KvServer
2. 若非 Leader 节点，拒绝并返回 Leader 地址 (HTTP 307 重定向)
3. Leader 的 KvServer 创建 LogEntry 并调用 raftNode.propose()
4. RaftNode 通过 RPC 将日志复制到多数节点
5. 当日志提交后，KvServer 唤醒等待中的客户端请求
```

##### 3. 线性化读取优化
```cpp
// 通过 Raft 的 Leader Lease 机制保证读取一致性
std::string KvServer::getLinearizable(const std::string& key) {
    // 1. 向 Raft 提交一个空操作（NOOP）确认 Leader 有效性
    int64_t readIndex = raftNode.getLastCommittedIndex();
    
    // 2. 等待状态机应用到这个索引
    while (stateMachine.getLastApplied() < readIndex) {
        std::this_thread::sleep_for(1ms);
    }
    
    // 3. 现在可以安全读取
    return stateMachine.get(key);
}
```

---

#### 三、关键交互场景示例

##### 场景1：客户端写入数据
```cpp
// 客户端代码示例
KvClient client("10.0.0.1:50051"); // 连接任意节点
client.set("name", "Alice"); 

// 内部发生：
// 1. KvServer 创建 LogEntry {term:5, index:102, op:SET, key:"name", value:"Alice"}
// 2. RaftNode 发送 AppendEntries RPC 给 Followers
// 3. 多数节点确认后提交日志
// 4. KvStateMachine 执行 database->put("name", "Alice")
```

##### 场景2：节点间心跳通信
```cpp
// Leader 定时发送心跳
void RaftNode::sendHeartbeat() {
    AppendEntriesReq req;
    req.term = currentTerm;
    req.leaderId = nodeId;
    req.prevLogIndex = lastLogIndex();
    
    for (auto& peer : peers) {
        rpcClient.sendAppendEntries(peer, req, [this](AppendEntriesResp resp) {
            if (resp.success) {
                updateNextIndex(peer.id, resp.matchIndex);
            }
        });
    }
}
```

##### 场景3：故障恢复后的日志同步
```cpp
// Follower 处理 AppendEntries RPC
void RaftNode::onAppendEntries(const AppendEntriesReq& req) {
    if (req.term < currentTerm) {
        return resp.success = false; // 拒绝旧 Leader 的请求
    }
    
    // 检查日志连续性
    if (log[req.prevLogIndex].term != req.prevLogTerm) {
        return resp.success = false; // 需要日志回溯
    }
    
    // 追加新日志并持久化
    log.insert(log.end(), req.entries.begin(), req.entries.end());
    storage.saveLogs(log);
    
    // 更新提交索引
    if (req.leaderCommit > commitIndex) {
        commitIndex = std::min(req.leaderCommit, lastLogIndex());
    }
}
```

---

### 四、线程模型与并发控制

##### 推荐线程架构
```
+----------------+     +-----------------+
|  RPC 接收线程  | --> | 任务队列 (MPMC) | --> | 工作线程池 |
+----------------+     +-----------------+     +------------+
       ^                                                 |
       |                                                 v
+----------------+                                 +------------+
| 定时器线程     |                                 | 状态机应用 |
| (选举/心跳)    |                                 | 线程       |
+----------------+                                 +------------+
```

##### 关键并发控制点
1. **RaftNode 状态变更锁**
   ```cpp
   std::mutex raftStateMutex; // 保护 currentTerm/votedFor/log 等状态
   ```
2. **提交索引原子变量**
   ```cpp
   std::atomic<int64_t> commitIndex{0}; // 无锁读取已提交索引
   ```
3. **客户端请求去重表**
   ```cpp
   std::shared_mutex clientTableMutex; // 读写锁优化
   std::unordered_map<std::string, int64_t> clientLastSeq;
   ```

---

### 五、配置管理建议

1. **节点配置文件格式** (`cluster.conf`)
   ```json
   {
     "nodes": [
       {"id": 1, "address": "10.0.0.1:50051"},
       {"id": 2, "address": "10.0.0.2:50051"},
       {"id": 3, "address": "10.0.0.3:50051"}
     ],
     "election_timeout": {"min": 150, "max": 300} // 单位 ms
   }
   ```

2. **动态成员变更** (Raft 扩展功能)
   ```cpp
   // 特殊日志类型用于配置变更
   LogEntry entry;
   entry.op = Operation::CONFIG_CHANGE;
   entry.data = serialize(newConfig); 
   raftNode.propose(entry);
   ```

--- 

// 用户客户端示例（使用 gRPC 协议）
class KvClient {
private:
    std::string currentLeader;  // 当前已知的 Leader 地址
    std::vector<std::string> servers; // 集群所有节点地址（用于重试）
    grpc::ChannelCredentials* creds;

public:
    KvClient(const std::vector<std::string>& serverAddrs) {
        servers = serverAddrs;
        creds = grpc::InsecureChannelCredentials();
        currentLeader = servers[0]; // 初始猜测第一个节点是 Leader
    }

    // 发送 SET 请求到集群
    bool set(const std::string& key, const std::string& value) {
        while (true) {
            // 创建 gRPC 通道连接当前 Leader
            auto channel = grpc::CreateChannel(currentLeader, creds);
            auto stub = kvservice::KvService::NewStub(channel);
            
            // 构建请求
            ClientRequest req;
            req.set_key(key);
            req.set_value(value);
            req.set_optype(OpType::SET);
            
            // 发送请求
            ClientResponse resp;
            grpc::ClientContext ctx; 
            auto status = stub->ClientCommand(&ctx, req, &resp);
            
            // 处理响应
            if (status.ok()) {
                if (resp.success()) {
                    return true; // 操作成功
                } else if (resp.error() == NOT_LEADER) {
                    // 更新 Leader 地址并重试
                    currentLeader = resp.leader_hint();
                    continue;
                }
            } else {
                // 网络故障，尝试其他节点
                rotateServer();
            }
        }
    }

    void rotateServer() {
        // 轮换到下一个节点地址
        std::rotate(servers.begin(), servers.begin()+1, servers.end());
        currentLeader = servers[0];
    }
};
